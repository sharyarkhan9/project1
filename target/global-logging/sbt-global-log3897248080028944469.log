[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-1)))[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Processing event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0manalysis location (C:\Users\Family\Desktop\Revature - Big Data\211213-Big-Data-Reston\week 7\Project1Ex\target\scala-2.11\zinc\inc_compile_2.11.zip,true)[0m
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 1 s, completed Jan 26, 2022 10:11:29 AM[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Done event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Users/Family/Desktop/Revature%20-%20Big%20Data/211213-Big-Data-Reston/week%207/Project1Ex/src/main/scala/Project1Ex.scala","languageId":"scala","version":1,"text":"import org.apache.spark.sql.hive.HiveContext\r\nimport org.apache.spark.SparkConf\r\nimport org.apache.spark.SparkContext\r\nimport org.apache.spark.SparkContext._\r\nimport org.apache.spark.sql.SQLContext\r\nimport java.sql.DriverManager\r\nimport java.sql.Connection\r\nimport java.util.Scanner\r\n\r\nobject Project1Ex {\r\n    def main(args: Array[String]): Unit = {\r\n        // This block of code is all necessary for spark/hive/hadoop\r\n        System.setSecurityManager(null)\r\n        System.setProperty(\"hadoop.home.dir\", \"C:\\\\hadoop\\\\\") // change if winutils.exe is in a different bin folder\r\n        val conf = new SparkConf()\r\n            .setMaster(\"local\") \r\n            .setAppName(\"Project1Ex\")    // Change to whatever app name you want\r\n        val sc = new SparkContext(conf)\r\n        sc.setLogLevel(\"ERROR\")\r\n        val hiveCtx = new HiveContext(sc)\r\n        import hiveCtx.implicits._\r\n\r\n        //This block to connect to mySQL\r\n        val driver = \"com.mysql.cj.jdbc.Driver\"\r\n        val url = \"jdbc:mysql://localhost:3306/p1\" // Modify for whatever port you are running your DB on\r\n        val username = \"root\"\r\n        val password = \"########\" // Update to include your password\r\n        var connection:Connection = null\r\n\r\n        Class.forName(driver)\r\n        connection = DriverManager.getConnection(url, username, password)\r\n\r\n        // Method to check login credentials\r\n        val adminCheck = login(connection)\r\n        if (adminCheck) {\r\n            println(\"Welcome Admin! Loading in data...\")\r\n        } else {\r\n            println(\"Welcome User! Loading in data...\")\r\n        }\r\n        \r\n\r\n        // Run method to insert Covid data. Only needs to be ran initially, then table data1 will be persisted.\r\n        insertCovidData(hiveCtx)\r\n\r\n\r\n        /*\r\n        * Here is where I would ask the user for input on what queries they would like to run, as well as\r\n        * method calls to run those queries. An example is below, top10DeathRates(hiveCtx) \r\n        * \r\n        */\r\n\r\n        top10DeathRates(hiveCtx)\r\n\r\n        sc.stop() // Necessary to close cleanly. Otherwise, spark will continue to run and run into problems.\r\n    }\r\n\r\n    // This method checks to see if a user-inputted username/password combo is part of a mySQL table.\r\n    // Returns true if admin, false if basic user, gets stuckl in a loop until correct combo is inputted (FIX)\r\n    def login(connection: Connection): Boolean = {\r\n        \r\n        while (true) {\r\n            val statement = connection.createStatement()\r\n            val statement2 = connection.createStatement()\r\n            println(\"Enter username: \")\r\n            var scanner = new Scanner(System.in)\r\n            var username = scanner.nextLine().trim()\r\n\r\n            println(\"Enter password: \")\r\n            var password = scanner.nextLine().trim()\r\n            val resultSet = statement.executeQuery(\"SELECT COUNT(*) FROM admin_accounts WHERE username='\"+username+\"' AND password='\"+password+\"';\")\r\n            while ( resultSet.next() ) {\r\n                if (resultSet.getString(1) == \"1\") {\r\n                    return true;\r\n                }\r\n            }\r\n\r\n            val resultSet2 = statement2.executeQuery(\"SELECT COUNT(*) FROM user_accounts WHERE username='\"+username+\"' AND password='\"+password+\"';\")\r\n            while ( resultSet2.next() ) {\r\n                if (resultSet2.getString(1) == \"1\") {\r\n                    return false;\r\n                }\r\n            }\r\n\r\n            println(\"Username/password combo not found. Try again!\")\r\n        }\r\n        return false\r\n    }\r\n\r\n    def insertCovidData(hiveCtx:HiveContext): Unit = {\r\n                //hiveCtx.sql(\"LOAD DATA LOCAL INPATH 'input/covid_19_data.txt' OVERWRITE INTO TABLE data1\")\r\n        //hiveCtx.sql(\"INSERT INTO data1 VALUES (1, 'date', 'California', 'US', 'update', 10, 1, 0)\")\r\n\r\n        // This statement creates a DataFrameReader from your file that you wish to pass in. We can infer the schema and retrieve\r\n        // column names if the first row in your csv file has the column names. If not wanted, remove those options. This can \r\n        // then be \r\n        val output = hiveCtx.read\r\n            .format(\"csv\")\r\n            .option(\"inferSchema\", \"true\")\r\n            .option(\"header\", \"true\")\r\n            .load(\"input/covid_19_data.csv\")\r\n        output.limit(15).show() // Prints out the first 15 lines of the dataframe\r\n\r\n        // output.registerTempTable(\"data2\") // This will create a temporary table from your dataframe reader that can be used for queries. \r\n\r\n        // These next three lines will create a temp view from the dataframe you created and load the data into a permanent table inside\r\n        // of Hadoop. Thus, we will have data persistence, and this code only needs to be ran once. Then, after the initializatio, this \r\n        // code as well as the creation of output will not be necessary.\r\n        output.createOrReplaceTempView(\"temp_data\")\r\n        hiveCtx.sql(\"CREATE TABLE IF NOT EXISTS data1 (SNo INT, ObservationDate STRING, Province_State STRING, Country_Region STRING, LastUpdate STRING, Confirmed INT, Deaths INT, Recovered INT)\")\r\n        hiveCtx.sql(\"INSERT INTO data1 SELECT * FROM temp_data\")\r\n        \r\n        // To query the data1 table. When we make a query, the result set ius stored using a dataframe. In order to print to the console, \r\n        // we can use the .show() method.\r\n        val summary = hiveCtx.sql(\"SELECT * FROM data1 LIMIT 10\")\r\n        summary.show()\r\n    }\r\n\r\n    def top10DeathRates(hiveCtx:HiveContext): Unit = {\r\n        val result = hiveCtx.sql(\"SELECT Province_State State, MAX(deaths)/MAX(confirmed) Death_Rate FROM data1 WHERE country_region='US' AND Province_State NOT LIKE '%,%' GROUP BY State ORDER BY Death_Rate DESC LIMIT 10\")\r\n        result.show()\r\n        result.write.csv(\"results/top10DeathRates\")\r\n    }\r\n}\r\n"}})[0m
