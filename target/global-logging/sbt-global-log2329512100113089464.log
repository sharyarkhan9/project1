[0m[[0m[0mdebug[0m] [0m[0m> Exec(run, Some(7dffbbc2-fd25-4095-9f02-7d1e2466d203), Some(CommandSource(console0)))[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / run[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/taskStart, {"taskId":{"id":"28","parents":[]},"eventTime":1643297659304,"message":"Compiling project1","dataKind":"compile-task","data":{"target":{"uri":"file:/C:/Users/Family/Desktop/Project1/#project1/Compile"}}})[0m
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/publishDiagnostics, {"textDocument":{"uri":"file:///C:/Users/Family/Desktop/Project1/src/main/scala/Project1.scala"},"buildTarget":{"uri":"file:/C:/Users/Family/Desktop/Project1/#project1/Compile"},"diagnostics":[],"reset":true})[0m
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/taskFinish, {"taskId":{"id":"28","parents":[]},"eventTime":1643297659369,"message":"Compiled project1","status":1,"dataKind":"compile-report","data":{"target":{"uri":"file:/C:/Users/Family/Desktop/Project1/#project1/Compile"},"errors":0,"warnings":0,"time":65}})[0m
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":3,"message":"running Project1 "})[0m
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":4,"message":"  Classpath:\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\job-16\\target\\bb68e414\\a417f45f\\project1_2.11-0.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\642eee43\\ffcc071c\\scala-library-2.11.12.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\01469e66\\95a8b862\\spark-core_2.11-2.3.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\5c622c00\\5f9e97a9\\spark-hive_2.11-2.3.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\fd0d4642\\457512ae\\mysql-connector-java-8.0.27.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\3d99959d\\3efb6678\\avro-1.7.7.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\c08b3868\\43e295fb\\avro-mapred-1.7.7-hadoop2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\77577381\\b1951865\\chill_2.11-0.8.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\203bff4b\\c8629223\\chill-java-0.8.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\f815fb07\\ba175208\\xbean-asm5-shaded-4.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\2531ba34\\321beac1\\hadoop-client-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\e33123cd\\3cc842a7\\spark-launcher_2.11-2.3.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\12bb68fa\\2413524c\\spark-kvstore_2.11-2.3.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\2182ed15\\86a5fdbf\\spark-network-common_2.11-2.3.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\0e86fdc0\\38dd04f3\\spark-network-shuffle_2.11-2.3.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\5f1a9446\\32697393\\spark-unsafe_2.11-2.3.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\51314927\\9831dc0b\\jets3t-0.9.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\4a35e762\\b80d806f\\curator-recipes-2.6.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\a4e371f9\\34c33ae9\\javax.servlet-api-3.1.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\445ebcaa\\066b4db5\\commons-lang3-3.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\c5295ae4\\139c3a45\\commons-math3-3.4.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\66823929\\1dddabdd\\jsr305-3.0.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\67299c8d\\be5b7d96\\slf4j-api-1.7.25.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\6ee9e529\\eaf89648\\jul-to-slf4j-1.7.16.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\7da0d29e\\29bbf076\\jcl-over-slf4j-1.7.16.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\a30f1c2f\\4d4c8d04\\log4j-1.2.17.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\b426d445\\15deb58f\\slf4j-log4j12-1.7.16.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\148fe6bc\\9eecf81e\\compress-lzf-1.0.3.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\51287d5c\\14d0e4ec\\snappy-java-1.1.2.6.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\20eb419f\\c52eac92\\lz4-java-1.4.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\b9443b63\\b81a9325\\zstd-jni-1.3.2-2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\921f58b7\\00fe3efc\\RoaringBitmap-0.5.11.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\2443d707\\9f624005\\commons-net-3.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\c10b7d72\\5a30c7ef\\json4s-jackson_2.11-3.2.11.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\9c28b24a\\db44d340\\jersey-client-2.22.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\1423c947\\baa2e9e6\\jersey-common-2.22.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\f1700847\\6136cfca\\jersey-server-2.22.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\c388e9fd\\7cc7f1e0\\jersey-container-servlet-2.22.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\2850d86e\\a3b33adf\\jersey-container-servlet-core-2.22.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\f4161b69\\eb10f320\\netty-all-4.1.17.Final.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\6e510d32\\de2710fe\\netty-3.9.9.Final.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\9e493d16\\ea2371fe\\stream-2.7.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\1d1cb747\\fae4c332\\metrics-core-3.1.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\96509abf\\821b4235\\metrics-jvm-3.1.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\9bb8ccff\\96fd7ba5\\metrics-json-3.1.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\f5545615\\8514dc8e\\metrics-graphite-3.1.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\49343b8c\\3c92579b\\jackson-databind-2.6.7.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\32772e4b\\c6076f32\\jackson-module-scala_2.11-2.6.7.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\d989936e\\407c9087\\ivy-2.4.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\0b1a6517\\1783d108\\oro-2.0.8.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\421cd41d\\c2cb318f\\pyrolite-4.13.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\9e573485\\d8d553df\\py4j-0.10.7.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\d50493f6\\97c5129d\\spark-tags_2.11-2.3.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\50960053\\0b4cdca5\\commons-crypto-1.0.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\2bac330e\\e703ea1a\\unused-1.0.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\a0ea1958\\9bcefa8d\\parquet-hadoop-bundle-1.6.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\79b8de34\\f79b4911\\spark-sql_2.11-2.3.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\12e7c2de\\e6d2f73f\\hive-exec-1.2.1.spark2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\fb5288e5\\a7a78db9\\hive-metastore-1.2.1.spark2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\0a9aad92\\e8fdb0c3\\commons-httpclient-3.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\8099a89b\\4b14e1bb\\calcite-avatica-1.2.0-incubating.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\2b3bd23f\\e09deace\\calcite-core-1.2.0-incubating.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\35c280da\\fc937339\\httpclient-4.5.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\05a28b02\\cae88928\\jackson-mapper-asl-1.9.13.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\d72e8d98\\840dd1b1\\commons-codec-1.10.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\3458a0c4\\759f1fa3\\joda-time-2.9.9.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\a661f723\\4d4e6003\\jodd-core-3.5.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\a3cce9b0\\cab6c04a\\datanucleus-core-3.2.10.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\42eeb477\\e81ff748\\libthrift-0.9.3.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\6848e7c1\\914889cb\\libfb303-0.9.3.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\1f16fef5\\d4f190b2\\derby-10.12.1.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\446bb26c\\074bafba\\protobuf-java-3.11.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\32c95120\\b1e82868\\jackson-core-asl-1.9.13.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\2ce440bc\\daed874d\\paranamer-2.8.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\0dad1b8b\\f5ea249b\\commons-compress-1.4.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\2f6fdfb8\\768d69cd\\avro-ipc-1.7.7.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\138df6cb\\89dac59e\\avro-ipc-1.7.7-tests.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\70bcc343\\aa7d91f0\\kryo-shaded-3.0.3.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\b520a0f1\\cd19c52a\\hadoop-common-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\081e1104\\ce2f5753\\hadoop-hdfs-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\7d6f2388\\a7cd1990\\hadoop-mapreduce-client-app-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\ee763b25\\fbbd2168\\hadoop-yarn-api-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\18a1b685\\cba1f8e3\\hadoop-mapreduce-client-core-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\1715d07b\\f9a48a67\\hadoop-mapreduce-client-jobclient-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\362c4b10\\3537fa30\\hadoop-annotations-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\4f40c189\\a66253a7\\leveldbjni-all-1.8.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\6658cc01\\a9c07688\\jackson-core-2.7.9.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\1000d7da\\bfe3aef6\\jackson-annotations-2.6.7.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\1a74abc8\\6cf7337a\\httpcore-4.4.7.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\f47191c6\\2359205f\\activation-1.1.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\9c901dff\\a9b4d3ad\\bcprov-jdk15on-1.52.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\39129b54\\172782bd\\java-xmlbuilder-1.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\d64aa841\\fd7ba014\\curator-framework-2.6.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\1ec9321b\\ac0f4693\\zookeeper-3.4.6.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\f1a34392\\fce55f79\\guava-16.0.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\7d592f75\\5d0d6e53\\json4s-core_2.11-3.2.11.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\ccef57b6\\819ce6ba\\javax.ws.rs-api-2.0.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\ff95a2db\\b48ff7bb\\hk2-api-2.4.0-b34.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\d2295934\\8987099e\\javax.inject-2.4.0-b34.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\b937d3dc\\fe834189\\hk2-locator-2.4.0-b34.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\749a853c\\3c8b0e1b\\javax.annotation-api-1.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\79aad059\\c7a5df02\\jersey-guava-2.22.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\85c5b5e7\\1b227ac8\\osgi-resource-locator-1.0.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\29872d11\\ccba4b6b\\jersey-media-jaxb-2.22.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\f39ec00b\\88e3e3d4\\validation-api-1.1.0.Final.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\2efc4657\\9e5bae49\\scala-reflect-2.11.12.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\c73be3a2\\33fdbe14\\jackson-module-paranamer-2.7.9.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\425fc0e2\\d3ada0ed\\univocity-parsers-2.5.9.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\62e3b257\\32cb1b31\\spark-sketch_2.11-2.3.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\3be04aed\\6fd26f3b\\spark-catalyst_2.11-2.3.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\55f7a176\\a1a88249\\orc-core-1.4.4-nohive.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\d184e7b1\\6872a5b2\\orc-mapreduce-1.4.4-nohive.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\e1a26eb1\\f463cf6f\\parquet-column-1.8.3.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\fd3cbb64\\25adc117\\parquet-hadoop-1.8.3.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\8298df0d\\cc24ed63\\arrow-vector-0.8.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\4b8816f8\\3a66980a\\commons-io-2.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\ef20499d\\d5598fe1\\commons-lang-2.6.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\d28f0506\\3daf3e98\\javolution-5.5.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\76214c0b\\819b13ba\\apache-log4j-extras-1.2.17.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\9f548fd1\\da64efa6\\antlr-runtime-3.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\39ba7086\\ea8d04b9\\ST4-4.0.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\b01c2017\\7c0a31c5\\JavaEWAH-0.3.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\e60e699c\\025d0b65\\snappy-0.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\1a2e6274\\c86c22c2\\stax-api-1.0.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\f12cae17\\b010f01b\\opencsv-2.3.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\4126b1f2\\944cb591\\bonecp-0.8.0.RELEASE.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\7d81b065\\77b61b6b\\commons-cli-1.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\41e2e419\\e02e153b\\commons-logging-1.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\21bcc6e7\\824b2c2c\\datanucleus-api-jdo-3.2.6.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\2c1b4424\\01150e05\\datanucleus-rdbms-3.2.9.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\d898bec7\\4ff37331\\commons-pool-1.5.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\1cbf85f2\\945cd2fb\\commons-dbcp-1.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\08d67071\\9aa09df7\\jdo-api-3.0.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\9b5ff01a\\b468721f\\calcite-linq4j-1.2.0-incubating.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\59390efd\\b473fadb\\eigenbase-properties-1.1.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\85a1aaef\\018a6ef1\\xz-1.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\373f6bdf\\130f6609\\minlog-1.3.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\c032efaf\\b6e52d08\\objenesis-2.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\42843813\\db21ae2a\\xmlenc-0.52.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\9721328a\\97a86a4a\\commons-collections-3.2.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\53deb6f2\\878f606c\\commons-configuration-1.6.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\c4ecb59e\\763cb43c\\gson-2.2.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\a1e04159\\7fb68989\\hadoop-auth-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\da700185\\a32fc6f4\\curator-client-2.6.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\6be5ddd2\\a444057d\\htrace-core-3.0.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\c2d08552\\2594821b\\jetty-util-6.1.26.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\23ff4d97\\fb9fe53c\\xercesImpl-2.9.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\0e77daa1\\b0b83e28\\hadoop-mapreduce-client-common-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\bd9b5501\\40eb4cbd\\hadoop-mapreduce-client-shuffle-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\1e3cbd7c\\3da4ab70\\hadoop-yarn-common-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\33731bca\\cc6d067e\\base64-2.3.8.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\349346d7\\07cd3981\\json4s-ast_2.11-3.2.11.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\2bc20baa\\f24c00fc\\scalap-2.11.12.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\adf5e32d\\535313f0\\hk2-utils-2.4.0-b34.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\97f33467\\cab97136\\aopalliance-repackaged-2.4.0-b34.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\6962c8e5\\e1132002\\javassist-3.18.1-GA.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\a0c2e99a\\7a89c252\\scala-parser-combinators_2.11-1.0.4.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\33d24ba8\\41a06ece\\janino-3.0.8.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\73f3068e\\db43bdcc\\commons-compiler-3.0.8.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\f0484137\\77ba1394\\antlr4-runtime-4.7.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\ab1ee812\\8e74b6a8\\aircompressor-0.8.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\f969f5f2\\e0af2267\\parquet-common-1.8.3.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\7a2312ce\\4ab8bdfc\\parquet-encoding-1.8.3.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\ac2f7bcc\\7ab57681\\parquet-format-2.3.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\0126cb57\\550c59b5\\parquet-jackson-1.8.3.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\9c5ff59f\\76d1102b\\arrow-format-0.8.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\718d20ff\\52c0ee9e\\arrow-memory-0.8.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\92607532\\51c5acda\\hppc-0.7.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\d09b8fa0\\71df4479\\flatbuffers-1.2.0-3f79e055.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\59c5e5d1\\241e3f10\\stringtemplate-3.2.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\79bdf83e\\21bc5909\\antlr-2.7.7.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\bcd29d20\\fdd0271d\\jta-1.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\9e5aeb6a\\0b51a591\\commons-digester-1.8.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\ea679478\\8c20ea4c\\commons-beanutils-core-1.8.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\7efa6ff4\\db6ccc7c\\apacheds-kerberos-codec-2.0.0-M15.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\49e45f5a\\b4c9e8b4\\xml-apis-1.3.04.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\6b05b9c7\\621358d1\\hadoop-yarn-client-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\021003fb\\f30cde4b\\hadoop-yarn-server-common-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\420cc915\\c2346a54\\jaxb-api-2.2.2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\26cf2c4f\\109c6655\\jackson-jaxrs-1.9.13.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\5cbb59f4\\a3eb1f2d\\jackson-xc-1.9.13.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\6c13b52d\\58fbae1a\\hadoop-yarn-server-nodemanager-2.6.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\f6e0ad05\\515bf8e2\\scala-compiler-2.11.12.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\dfcc77bd\\ed29cbc9\\javax.inject-1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\064f2e53\\d0579944\\commons-beanutils-1.7.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\f789686d\\b930484b\\apacheds-i18n-2.0.0-M15.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\f4471c8e\\8f14b031\\api-asn1-api-1.0.0-M20.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\82e249e1\\16813a4c\\api-util-1.0.0-M20.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\1eaf0e04\\973fef91\\jline-0.9.94.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\36cc741c\\00a216bb\\guice-3.0.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\249fa52a\\6f5c1009\\stax-api-1.0-2.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\57d51d43\\4d8b306b\\jettison-1.1.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\7042abe7\\72063af4\\scala-xml_2.11-1.0.5.jar\n\tC:\\Users\\Family\\Desktop\\Project1\\target\\bg-jobs\\sbt_8c090140\\target\\98d52ffe\\849273b3\\aopalliance-1.0.jar"})[0m
[0m[[0m[0mdebug[0m] [0m[0mlsp-definition json request: {"textDocument":{"uri":"file:///c%3A/Users/Family/Desktop/Project1/src/main/scala/Project1.scala"},"position":{"line":50,"character":59}}[0m
[0m[[0m[0mdebug[0m] [0m[0mlsp-definition found line:                 println("enter the username which you would like to delete: ")[0m
[0m[[0m[0mdebug[0m] [0m[0msymbol would[0m
[0m[[0m[0mdebug[0m] [0m[0mlsp-definition potentials: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mlsp-definition locations Vector()[0m
[0m[[0m[0mdebug[0m] [0m[0mlsp-definition json request: {"textDocument":{"uri":"file:///c%3A/Users/Family/Desktop/Project1/src/main/scala/Project1.scala"},"position":{"line":50,"character":59}}[0m
[0m[[0m[0mdebug[0m] [0m[0mlsp-definition found line:                 println("enter the username which you would like to delete: ")[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: $/cancelRequest: JsonRpcNotificationMessage(2.0, $/cancelRequest, {"id":20})[0m
[0m[[0m[0mdebug[0m] [0m[0mlsp-definition json request: {"textDocument":{"uri":"file:///c%3A/Users/Family/Desktop/Project1/src/main/scala/Project1.scala"},"position":{"line":50,"character":58}}[0m
[0m[[0m[0mdebug[0m] [0m[0mlsp-definition found line:                 println("enter the username which you would like to delete: ")[0m
[0m[[0m[0mdebug[0m] [0m[0msymbol would[0m
[0m[[0m[0mdebug[0m] [0m[0mlsp-definition potentials: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mlsp-definition locations Vector()[0m
[0m[[0m[0mdebug[0m] [0m[0msymbol would[0m
[0m[[0m[0mdebug[0m] [0m[0mlsp-definition potentials: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mlsp-definition locations Vector()[0m
[0m[[0m[31merror[0m] [0m[0morg.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.orElse(Option.scala:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.insertCovidData(Project1.scala:134)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.main(Project1.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1.main(Project1.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.orElse(Option.scala:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.insertCovidData(Project1.scala:134)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.main(Project1.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1.main(Project1.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.orElse(Option.scala:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.insertCovidData(Project1.scala:134)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.main(Project1.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1.main(Project1.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.lang.reflect.InvocationTargetException[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.orElse(Option.scala:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.insertCovidData(Project1.scala:134)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.main(Project1.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1.main(Project1.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------[0m
[0m[[0m[31merror[0m] [0m[0mjava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.sql.DriverManager.getConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.sql.DriverManager.getConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.orElse(Option.scala:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.insertCovidData(Project1.scala:134)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.main(Project1.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1.main(Project1.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	... 132 more[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\Users\Family\Desktop\Project1\metastore_db.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	... 129 more[0m
[0m[[0m[31merror[0m] [0m[0m------[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mNestedThrowables:[0m
[0m[[0m[31merror[0m] [0m[0mjava.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------[0m
[0m[[0m[31merror[0m] [0m[0mjava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.sql.DriverManager.getConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.sql.DriverManager.getConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.orElse(Option.scala:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.insertCovidData(Project1.scala:134)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.main(Project1.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1.main(Project1.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	... 132 more[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\Users\Family\Desktop\Project1\metastore_db.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	... 129 more[0m
[0m[[0m[31merror[0m] [0m[0m------[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:436)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.orElse(Option.scala:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.insertCovidData(Project1.scala:134)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.main(Project1.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1.main(Project1.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------[0m
[0m[[0m[31merror[0m] [0m[0mjava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.sql.DriverManager.getConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.sql.DriverManager.getConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.orElse(Option.scala:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.insertCovidData(Project1.scala:134)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.main(Project1.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1.main(Project1.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	... 132 more[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\Users\Family\Desktop\Project1\metastore_db.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	... 129 more[0m
[0m[[0m[31merror[0m] [0m[0m------[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.GeneratedConstructorAccessor252.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.orElse(Option.scala:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.insertCovidData(Project1.scala:134)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.main(Project1.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1.main(Project1.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.sql.DriverManager.getConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.sql.DriverManager.getConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.orElse(Option.scala:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.insertCovidData(Project1.scala:134)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.main(Project1.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1.main(Project1.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.sql.DriverManager.getConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.sql.DriverManager.getConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.orElse(Option.scala:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.insertCovidData(Project1.scala:134)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.main(Project1.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1.main(Project1.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\Users\Family\Desktop\Project1\metastore_db.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.sql.DriverManager.getConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.sql.DriverManager.getConnection(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.security.AccessController.doPrivileged(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)[0m
[0m[[0m[31merror[0m] [0m[0m	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Constructor.newInstance(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.orElse(Option.scala:289)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.insertCovidData(Project1.scala:134)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1$.main(Project1.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at Project1.main(Project1.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Unknown Source)[0m
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":1,"message":"org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat Project1$.insertCovidData(Project1.scala:134)\r\n\tat Project1$.main(Project1.scala:77)\r\n\tat Project1.main(Project1.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sbt.Run.invokeMain(Run.scala:143)\r\n\tat sbt.Run.execute$1(Run.scala:93)\r\n\tat sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)\r\n\tat sbt.Run$.executeSuccess(Run.scala:186)\r\n\tat sbt.Run.runWithLoader(Run.scala:120)\r\n\tat sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)\r\n\tat sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat Project1$.insertCovidData(Project1.scala:134)\r\n\tat Project1$.main(Project1.scala:77)\r\n\tat Project1.main(Project1.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sbt.Run.invokeMain(Run.scala:143)\r\n\tat sbt.Run.execute$1(Run.scala:93)\r\n\tat sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)\r\n\tat sbt.Run$.executeSuccess(Run.scala:186)\r\n\tat sbt.Run.runWithLoader(Run.scala:120)\r\n\tat sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)\r\n\tat sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat Project1$.insertCovidData(Project1.scala:134)\r\n\tat Project1$.main(Project1.scala:77)\r\n\tat Project1.main(Project1.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sbt.Run.invokeMain(Run.scala:143)\r\n\tat sbt.Run.execute$1(Run.scala:93)\r\n\tat sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)\r\n\tat sbt.Run$.executeSuccess(Run.scala:186)\r\n\tat sbt.Run.runWithLoader(Run.scala:120)\r\n\tat sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)\r\n\tat sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat Project1$.insertCovidData(Project1.scala:134)\r\n\tat Project1$.main(Project1.scala:77)\r\n\tat Project1.main(Project1.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sbt.Run.invokeMain(Run.scala:143)\r\n\tat sbt.Run.execute$1(Run.scala:93)\r\n\tat sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)\r\n\tat sbt.Run$.executeSuccess(Run.scala:186)\r\n\tat sbt.Run.runWithLoader(Run.scala:120)\r\n\tat sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)\r\n\tat sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\r\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\r\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat Project1$.insertCovidData(Project1.scala:134)\r\n\tat Project1$.main(Project1.scala:77)\r\n\tat Project1.main(Project1.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sbt.Run.invokeMain(Run.scala:143)\r\n\tat sbt.Run.execute$1(Run.scala:93)\r\n\tat sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)\r\n\tat sbt.Run$.executeSuccess(Run.scala:186)\r\n\tat sbt.Run.runWithLoader(Run.scala:120)\r\n\tat sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)\r\n\tat sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n\t... 132 more\r\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\\Users\\Family\\Desktop\\Project1\\metastore_db.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\r\n\t... 129 more\r\n------\r\n\nNestedThrowables:\njava.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\r\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\r\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat Project1$.insertCovidData(Project1.scala:134)\r\n\tat Project1$.main(Project1.scala:77)\r\n\tat Project1.main(Project1.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sbt.Run.invokeMain(Run.scala:143)\r\n\tat sbt.Run.execute$1(Run.scala:93)\r\n\tat sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)\r\n\tat sbt.Run$.executeSuccess(Run.scala:186)\r\n\tat sbt.Run.runWithLoader(Run.scala:120)\r\n\tat sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)\r\n\tat sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n\t... 132 more\r\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\\Users\\Family\\Desktop\\Project1\\metastore_db.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\r\n\t... 129 more\r\n------\r\n\r\n\tat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:436)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat Project1$.insertCovidData(Project1.scala:134)\r\n\tat Project1$.main(Project1.scala:77)\r\n\tat Project1.main(Project1.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sbt.Run.invokeMain(Run.scala:143)\r\n\tat sbt.Run.execute$1(Run.scala:93)\r\n\tat sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)\r\n\tat sbt.Run$.executeSuccess(Run.scala:186)\r\n\tat sbt.Run.runWithLoader(Run.scala:120)\r\n\tat sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)\r\n\tat sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\r\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\r\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat Project1$.insertCovidData(Project1.scala:134)\r\n\tat Project1$.main(Project1.scala:77)\r\n\tat Project1.main(Project1.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sbt.Run.invokeMain(Run.scala:143)\r\n\tat sbt.Run.execute$1(Run.scala:93)\r\n\tat sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)\r\n\tat sbt.Run$.executeSuccess(Run.scala:186)\r\n\tat sbt.Run.runWithLoader(Run.scala:120)\r\n\tat sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)\r\n\tat sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n\t... 132 more\r\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\\Users\\Family\\Desktop\\Project1\\metastore_db.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\r\n\t... 129 more\r\n------\r\n\r\n\tat sun.reflect.GeneratedConstructorAccessor252.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)\r\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)\r\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat Project1$.insertCovidData(Project1.scala:134)\r\n\tat Project1$.main(Project1.scala:77)\r\n\tat Project1.main(Project1.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sbt.Run.invokeMain(Run.scala:143)\r\n\tat sbt.Run.execute$1(Run.scala:93)\r\n\tat sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)\r\n\tat sbt.Run$.executeSuccess(Run.scala:186)\r\n\tat sbt.Run.runWithLoader(Run.scala:120)\r\n\tat sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)\r\n\tat sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\r\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\r\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat Project1$.insertCovidData(Project1.scala:134)\r\n\tat Project1$.main(Project1.scala:77)\r\n\tat Project1.main(Project1.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sbt.Run.invokeMain(Run.scala:143)\r\n\tat sbt.Run.execute$1(Run.scala:93)\r\n\tat sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)\r\n\tat sbt.Run$.executeSuccess(Run.scala:186)\r\n\tat sbt.Run.runWithLoader(Run.scala:120)\r\n\tat sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)\r\n\tat sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@380b10ab, see the next exception for details.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\r\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\r\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat Project1$.insertCovidData(Project1.scala:134)\r\n\tat Project1$.main(Project1.scala:77)\r\n\tat Project1.main(Project1.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sbt.Run.invokeMain(Run.scala:143)\r\n\tat sbt.Run.execute$1(Run.scala:93)\r\n\tat sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)\r\n\tat sbt.Run$.executeSuccess(Run.scala:186)\r\n\tat sbt.Run.runWithLoader(Run.scala:120)\r\n\tat sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)\r\n\tat sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\\Users\\Family\\Desktop\\Project1\\metastore_db.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\r\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\r\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:114)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:385)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:287)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.<init>(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)\r\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:432)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat Project1$.insertCovidData(Project1.scala:134)\r\n\tat Project1$.main(Project1.scala:77)\r\n\tat Project1.main(Project1.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sbt.Run.invokeMain(Run.scala:143)\r\n\tat sbt.Run.execute$1(Run.scala:93)\r\n\tat sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)\r\n\tat sbt.Run$.executeSuccess(Run.scala:186)\r\n\tat sbt.Run.runWithLoader(Run.scala:120)\r\n\tat sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)\r\n\tat sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)"})[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrun[0m) org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;[0m
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":1,"message":"(Compile / \u001b[31mrun\u001b[0m) org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;"})[0m
[0m[[0m[31merror[0m] [0m[0mTotal time: 67 s (01:07), completed Jan 27, 2022 10:35:26 AM[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
